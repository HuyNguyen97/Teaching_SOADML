{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVRG and SAGA Algorithms\n",
    "\n",
    "Our goal is to minimize a convex function $f : \\mathbb{R}^p \\to \\mathbb{R}$ of the form\n",
    "$$ f: \\theta \\to \\sum_{i=1}^n f_i(\\theta) $$\n",
    "where $f_1, \\ldots, f_n : \\mathbb{R}^p \\to \\mathbb{R}$ are smooth convex functions. In Machine Learning, such functions appear all the time with $f_i(\\theta)$ being the loss of the $i$-th datum for parameter $\\theta$.\n",
    "\n",
    "In the previous Jupyter notebook, we used Stochastic Gradient Descent to efficiently solve this minimization program. The problem we faced was the high variance of the gradients: although $\\nabla f_i(\\theta)$ is an unbiased estimator of $\\nabla f(\\theta)$ when datum $i$ is chosen uniformly at random, its variance can be quite high. This motivated several work to find (un)biased estimators of the gradient with reduced variance. In this class, we will focus on two such algorithms: SVRG and SAGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVRG: Stochastic Variance Reduced Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intuition behind SVRG\n",
    "\n",
    "The idea is to reuse the basic SGD iteration, but with a modified version of the gradient. At each iteration $k$, we will pick a datum $i$ at random, compute the gradient for datum $i$ ($\\nabla f_i(\\theta_k)$), and modify this gradient by a zero-mean vector $- \\nabla f_i(\\hat\\theta) + \\nabla f(\\hat\\theta)$ where $\\hat\\theta$ is a previously computed point.\n",
    "\n",
    "In other words, SVRG \"remembers\" a full gradient from the past $\\nabla f(\\hat\\theta)$ and uses this information to reduce the variance of $\\nabla f_i(\\theta_k)$.\n",
    "\n",
    "The iterations write:\n",
    "* Initialize $\\hat\\theta$ at random\n",
    "* Do until convergence:\n",
    "    * Compute full gradient at $\\hat\\theta$: $\\nabla f(\\hat\\theta)$\n",
    "    * Put $\\theta_0 = \\hat\\theta$\n",
    "    * For $k$ between $1$ and $K$:\n",
    "        * Choose $i \\in [1 ; n]$ uniformly at random\n",
    "        * $\\theta_{k+1} = \\theta_k - \\gamma \\left[ \\nabla f_i(\\theta_k) - \\nabla f_i(\\hat\\theta) + \\nabla f(\\hat\\theta) \\right]$\n",
    "    * Put $\\hat\\theta = \\frac{1}{K} \\theta_k$\n",
    "    \n",
    "__Choice of step-size $\\gamma$:__ Can be taken constant and quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first example: least squares\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$. We wish to minimize the function\n",
    "$$f(\\theta) = \\frac{1}{2}\\|y - X\\theta\\|^2.$$\n",
    "\n",
    "Here, we suppose that the observations $y$ were obtained by linear combination of data $X$, plus some noise $\\sigma \\xi$:\n",
    "$$ y = X w + \\sigma \\xi $$\n",
    "with $\\sigma > 0$ noise level and $\\xi \\sim \\mathcal{N}(0,I)$.\n",
    "\n",
    "We know that the minimum of $f$ is attained at\n",
    "$$\\theta^* = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the data\n",
    "n, p = 10, 5\n",
    "\n",
    "X = np.random.randn(n,p)\n",
    "w = np.random.randn(p)\n",
    "\n",
    "sigma = 1.\n",
    "xi = np.random.randn(n)\n",
    "\n",
    "y = X.dot(w) + sigma*xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function f and its gradient\n",
    "def f(theta):\n",
    "    '''Return the least squares error at point `theta`.'''\n",
    "    return 0.5 * np.linalg.norm(y - X.dot(theta))**2\n",
    "\n",
    "def full_grad_f(theta):\n",
    "    '''Return the full gradient of least squares error at point `theta`.'''\n",
    "    return - X.T.dot(y - X.dot(theta))\n",
    "\n",
    "def grad_f(theta, i):\n",
    "    '''Return the gradient of least squares error at point `theta` for datum `i`.'''\n",
    "    return - X[i]*(y[i] - X[i].dot(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum of f\n",
    "theta_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print('The minimum value of function `f` is =', f(theta_star))\n",
    "print('It is attained at point theta^* =', theta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to be run\n",
    "max_iter = 100\n",
    "max_iter_inner = 2*n\n",
    "\n",
    "# Lists to save the sequences theta_hat, f(theta_hat), theta\n",
    "theta_hat_history = []\n",
    "f_history = []\n",
    "theta_history = np.zeros((max_iter_inner, p))\n",
    "\n",
    "# Step size\n",
    "step_size = 0.005\n",
    "\n",
    "# Random initial point\n",
    "theta_hat = np.random.randn(p)\n",
    "\n",
    "# SVRG Iterations\n",
    "for t in range(max_iter):\n",
    "    \n",
    "    full_gradient_hat = full_grad_f(theta_hat)\n",
    "    theta = theta_hat\n",
    "    for k in range(max_iter_inner):\n",
    "        i = np.random.randint(0, n)\n",
    "        theta = # TODO\n",
    "        theta_history[k,:] = theta\n",
    "    \n",
    "    theta_hat = np.mean(theta_history, axis=0)\n",
    "    \n",
    "    theta_hat_history.append(theta_hat)\n",
    "    f_history.append(f(theta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vizualize the convergence of SVRG in terms of:\n",
    "* The sequence $f(\\theta_k) - f(\\theta^*) \\rightarrow 0$\n",
    "* The sequence $\\theta_k \\rightarrow \\theta^*$\n",
    "\n",
    "We will compare with the SGD approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "max_iter = 200*n\n",
    "\n",
    "# Lists to save the sequences theta, f(theta), theta_averaged, f(theta_averaged)\n",
    "theta_SGD_history = []\n",
    "f_SGD_history = []\n",
    "theta_averaged_history = []\n",
    "f_averaged_history = []\n",
    "\n",
    "# Step size\n",
    "L = np.linalg.norm(X.T.dot(X), ord=2)\n",
    "step_size = 1. / L\n",
    "\n",
    "# Random initial point\n",
    "theta = np.random.randn(p)\n",
    "theta_averaged = np.random.randn(p)\n",
    "\n",
    "# Gradient Descent Iterations\n",
    "for t in range(max_iter):\n",
    "    i = np.random.randint(0, n)\n",
    "    \n",
    "    theta = theta - step_size * grad_f(theta, i)\n",
    "    theta_averaged = (t*theta_averaged + theta)/(t+1)\n",
    "    \n",
    "    if t%(2*n) == 0:\n",
    "        theta_SGD_history.append(theta)\n",
    "        f_SGD_history.append(f(theta))\n",
    "        theta_averaged_history.append(theta_averaged)\n",
    "        f_averaged_history.append(f(theta_averaged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now plot the convergence in terms of values of `f`\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.semilogy(f_history-f(theta_star), label='SVRG', lw=7)\n",
    "plt.semilogy(f_SGD_history-f(theta_star), label='SGD', lw=7)\n",
    "plt.semilogy(f_averaged_history-f(theta_star), label='Averaged SGD', lw=7)\n",
    "plt.grid(ls=':')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Number of Iterations', fontsize=25)\n",
    "plt.ylabel('Approximation Error', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now plot the convergence in terms of the norm of `theta`\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.semilogy([np.linalg.norm(theta - theta_star) for theta in theta_hat_history], label='SVRG', lw=7)\n",
    "plt.semilogy([np.linalg.norm(theta - theta_star) for theta in theta_SGD_history], label='SGD', lw=7)\n",
    "plt.semilogy([np.linalg.norm(theta - theta_star) for theta in theta_averaged_history], label='Averaged SGD', lw=7)\n",
    "plt.grid(ls=':')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Number of Iterations', fontsize=25)\n",
    "plt.ylabel('Approximation Error', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second example: logistic regression\n",
    "\n",
    "We have data $x_1, \\ldots, x_n \\in \\mathbb{R}^n$ that belong to one of the two classes $1$ or $-1$, and we write $y_i$ th class of datum $x_i$. We suppose that we observe noisy clusters that are originally linearly separable, i.e.\n",
    "$$ y_i = \\text{sign} \\left( w^T x_i + \\sigma \\xi_i \\right) $$\n",
    "where $\\sigma > 0$ is the noise level and $\\xi_1, \\ldots, \\xi_n \\sim \\mathcal{N}(0,1)$ are i.i.d.\n",
    "\n",
    "We want to learn a logistic regressor on this data, i.e. we want to find $\\theta^* \\in \\mathbb{R}^p$ minimizing\n",
    "$$ f(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\log\\left( 1 + e^{-y_i \\theta^T x_i} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the data\n",
    "n, p = 100, 5\n",
    "\n",
    "X = np.random.randn(n, p)\n",
    "w = np.random.randn(p)\n",
    "\n",
    "sigma = 1\n",
    "xi = np.random.randn(n)\n",
    "\n",
    "y = np.sign(X.dot(w) + sigma*xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta):\n",
    "    '''Return the logistic loss for parameters `theta`.'''\n",
    "    return np.mean(np.log(1.0 + np.exp(-y*X.dot(theta))))\n",
    "\n",
    "def full_grad_f(theta):\n",
    "    '''Return the full gradient of the logistic loss `f` at point `theta`.'''\n",
    "    return (1./n)*(-y/(1 + np.exp(y * X.dot(theta)))).dot(X)\n",
    "\n",
    "def grad_f(theta, i):\n",
    "    '''Return the gradient of the logistic loss `f` at point `theta` for datum `i`.'''\n",
    "    return (1./n)*(-y[i]/(1 + np.exp(y[i] * X[i].dot(theta))))*X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to be run\n",
    "max_iter = 100\n",
    "max_iter_inner = 2*n\n",
    "\n",
    "# Lists to save the sequences theta_hat, f(theta_hat), theta\n",
    "theta_hat_history = []\n",
    "f_history = []\n",
    "theta_history = np.zeros((max_iter_inner, p))\n",
    "\n",
    "# Step size\n",
    "step_size = 0.05\n",
    "\n",
    "# Random initial point\n",
    "theta_hat = np.random.randn(p)\n",
    "\n",
    "# SVRG Iterations\n",
    "for t in range(max_iter):\n",
    "    \n",
    "    full_gradient_hat = full_grad_f(theta_hat)\n",
    "    theta = theta_hat\n",
    "    for k in range(max_iter_inner):\n",
    "        i = np.random.randint(0, n)\n",
    "        theta = # TODO\n",
    "        theta_history[k,:] = theta\n",
    "    \n",
    "    theta_hat = np.mean(theta_history, axis=0)\n",
    "    \n",
    "    theta_hat_history.append(theta_hat)\n",
    "    f_history.append(f(theta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to be run\n",
    "max_iter = 200*n\n",
    "\n",
    "# Lists to save the sequences theta, f(theta), theta_averaged, f(theta_averaged)\n",
    "theta_SGD_history = []\n",
    "f_SGD_history = []\n",
    "theta_averaged_history = []\n",
    "f_averaged_history = []\n",
    "\n",
    "# Step size\n",
    "L = np.linalg.norm((1./n)*X.T.dot(X), ord=2)\n",
    "step_size = 1./L\n",
    "\n",
    "# Random initial point\n",
    "theta = np.random.randn(p)\n",
    "\n",
    "# Gradient Descent Iterations\n",
    "for t in range(max_iter):\n",
    "    i = np.random.randint(0, n)\n",
    "    \n",
    "    theta = theta - step_size * grad_f(theta, i)\n",
    "    theta_averaged = (t*theta_averaged + theta)/(t+1)\n",
    "    \n",
    "    if t%(2*n) == 0:\n",
    "        theta_SGD_history.append(theta)\n",
    "        f_SGD_history.append(f(theta))\n",
    "        theta_averaged_history.append(theta_averaged)\n",
    "        f_averaged_history.append(f(theta_averaged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the approximative minimum value of `f`\n",
    "min_f = np.min([np.min(f_history), np.min(f_SGD_history), np.min(f_averaged_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now plot the convergence in terms of values of `f`\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.semilogy(f_history-min_f, label='SVRG', lw=7)\n",
    "plt.semilogy(f_SGD_history-min_f, label='SGD', lw=7)\n",
    "plt.semilogy(f_averaged_history-min_f, label='Averaged SGD', lw=7)\n",
    "plt.grid(ls=':')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Number of Iterations', fontsize=25)\n",
    "plt.ylabel('Approximation Error', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SAGA: Stochastic Average Gradient Augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intuition behind SAGA\n",
    "\n",
    "As in SVRG, SAGA will \"remember\" the past. But instead of fixing the past to be at a point $\\hat\\theta$, we here choose the reuse the last computed gradient.\n",
    "\n",
    "More precisely, instead of modifying the gradient with $- \\nabla f_i (\\hat \\theta) + \\nabla f(\\hat\\theta)$, we modify it with $- \\nabla f_i (\\hat \\theta_i) + \\frac{1}{n} \\sum_{j=1}^n \\nabla f_j (\\hat \\theta_j)$ where $\\hat\\theta_j$ is the last computed gradient for datum $j$. If datum $j$ has not been used yet, we can simply put $\\nabla f_j(\\hat\\theta_j) = 0$.\n",
    "\n",
    "The iterations write:\n",
    "* Initialize $\\theta_0$ at random, put $\\hat g_1 = \\ldots = \\hat g_n = 0$ and $\\bar g = 0$\n",
    "* For $k$ between $0$ and $K-1$:\n",
    "    * Choose $i \\in [1 ; n]$ uniformly at random\n",
    "    * Compute and store gradient at $\\theta_k$ for datum `i`: $g_i = \\nabla f_i(\\theta_k)$\n",
    "    * Compute `SAGA`gradient: $G = g_i - \\hat g_i + \\bar g$\n",
    "    * $\\theta_{k+1} = \\theta_k - \\gamma G$\n",
    "    * Update mean gradient: $\\bar g = \\bar g + \\frac{1}{n} \\left( g_i - \\hat g_i \\right)$\n",
    "    * Update stored gradients: $\\hat g_i = g_i$\n",
    "    \n",
    "__Choice of step-size $\\gamma$:__ Can be taken constant equal to $\\frac{1}{3L}$ where $L$ is the smothness constant of the $f_i$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first example: least squares\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$. We wish to minimize the function\n",
    "$$f(\\theta) = \\frac{1}{2}\\|y - X\\theta\\|^2.$$\n",
    "\n",
    "Here, we suppose that the observations $y$ were obtained by linear combination of data $X$, plus some noise $\\sigma \\xi$:\n",
    "$$ y = X w + \\sigma \\xi $$\n",
    "with $\\sigma > 0$ noise level and $\\xi \\sim \\mathcal{N}(0,I)$.\n",
    "\n",
    "We know that the minimum of $f$ is attained at\n",
    "$$\\theta^* = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the data\n",
    "n, p = 10, 5\n",
    "\n",
    "X = np.random.randn(n,p)\n",
    "w = np.random.randn(p)\n",
    "\n",
    "sigma = 1.\n",
    "xi = np.random.randn(n)\n",
    "\n",
    "y = X.dot(w) + sigma*xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function f and its gradient\n",
    "def f(theta):\n",
    "    '''Return the least squares error at point `theta`.'''\n",
    "    return 0.5 * np.linalg.norm(y - X.dot(theta))**2\n",
    "\n",
    "def full_grad_f(theta):\n",
    "    '''Return the full gradient of least squares error at point `theta`.'''\n",
    "    return - X.T.dot(y - X.dot(theta))\n",
    "\n",
    "def grad_f(theta, i):\n",
    "    '''Return the gradient of least squares error at point `theta` for datum `i`.'''\n",
    "    return - X[i]*(y[i] - X[i].dot(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum of f\n",
    "theta_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print('The minimum value of function `f` is =', f(theta_star))\n",
    "print('It is attained at point theta^* =', theta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to be run\n",
    "max_iter = 200*n\n",
    "\n",
    "# Lists to save the sequences theta_hat, f(theta_hat), theta\n",
    "theta_SAGA_history = []\n",
    "f_history = []\n",
    "\n",
    "# Stores the gradients and the mean gradient\n",
    "gradients = np.zeros((n,p))\n",
    "mean_gradient = np.zeros(p)\n",
    "\n",
    "# Step size\n",
    "L = np.linalg.norm(X.T.dot(X), ord=2)\n",
    "step_size = 1. / (3*L)\n",
    "\n",
    "# Random initial point\n",
    "theta = np.random.randn(p)\n",
    "\n",
    "# SAGA Iterations\n",
    "for t in range(max_iter):\n",
    "    i = np.random.randint(0, n)\n",
    "    \n",
    "    # Computation of Gradients\n",
    "    gradient_i = # TODO\n",
    "    G = # TODO\n",
    "    \n",
    "    # Gradient Step\n",
    "    theta = # TODO\n",
    "    if t%(2*n) == 0:\n",
    "        theta_SAGA_history.append(theta)\n",
    "        f_history.append(f(theta))\n",
    "    \n",
    "    # Storage\n",
    "    mean_gradient += # TODO\n",
    "    gradients[i] = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vizualize the convergence of SAGA in terms of:\n",
    "* The sequence $f(\\theta_k) - f(\\theta^*) \\rightarrow 0$\n",
    "* The sequence $\\theta_k \\rightarrow \\theta^*$\n",
    "\n",
    "We will compare with the SVRG approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVRG\n",
    "max_iter = 100\n",
    "max_iter_inner = 2*n\n",
    "\n",
    "# Lists to save the sequences theta_hat, f(theta_hat), theta\n",
    "theta_hat_history = []\n",
    "f_SVRG_history = []\n",
    "theta_history = np.zeros((max_iter_inner, p))\n",
    "\n",
    "# Step size\n",
    "step_size = 0.005\n",
    "\n",
    "# Random initial point\n",
    "theta_hat = np.random.randn(p)\n",
    "\n",
    "# SVRG Iterations\n",
    "for t in range(max_iter):\n",
    "    \n",
    "    full_gradient_hat = full_grad_f(theta_hat)\n",
    "    theta = theta_hat\n",
    "    for k in range(max_iter_inner):\n",
    "        i = np.random.randint(0, n)\n",
    "        theta = # TODO\n",
    "        theta_history[k,:] = theta\n",
    "    \n",
    "    theta_hat = np.mean(theta_history, axis=0)\n",
    "    \n",
    "    theta_hat_history.append(theta_hat)\n",
    "    f_SVRG_history.append(f(theta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now plot the convergence in terms of values of `f`\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.semilogy(f_SVRG_history-f(theta_star), label='SVRG', lw=7)\n",
    "plt.semilogy(f_history-f(theta_star), label='SAGA', lw=7)\n",
    "plt.grid(ls=':')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Number of Iterations', fontsize=25)\n",
    "plt.ylabel('Approximation Error', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now plot the convergence in terms of the norm of `theta`\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.semilogy([np.linalg.norm(theta - theta_star) for theta in theta_hat_history], label='SVRG', lw=7)\n",
    "plt.semilogy([np.linalg.norm(theta - theta_star) for theta in theta_SAGA_history], label='SAGA', lw=7)\n",
    "plt.grid(ls=':')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Number of Iterations', fontsize=25)\n",
    "plt.ylabel('Approximation Error', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second example: logistic regression\n",
    "\n",
    "We have data $x_1, \\ldots, x_n \\in \\mathbb{R}^n$ that belong to one of the two classes $1$ or $-1$, and we write $y_i$ th class of datum $x_i$. We suppose that we observe noisy clusters that are originally linearly separable, i.e.\n",
    "$$ y_i = \\text{sign} \\left( w^T x_i + \\sigma \\xi_i \\right) $$\n",
    "where $\\sigma > 0$ is the noise level and $\\xi_1, \\ldots, \\xi_n \\sim \\mathcal{N}(0,1)$ are i.i.d.\n",
    "\n",
    "We want to learn a logistic regressor on this data, i.e. we want to find $\\theta^* \\in \\mathbb{R}^p$ minimizing\n",
    "$$ f(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\log\\left( 1 + e^{-y_i \\theta^T x_i} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the data\n",
    "n, p = 100, 5\n",
    "\n",
    "X = np.random.randn(n, p)\n",
    "w = np.random.randn(p)\n",
    "\n",
    "sigma = 1\n",
    "xi = np.random.randn(n)\n",
    "\n",
    "y = np.sign(X.dot(w) + sigma*xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta):\n",
    "    '''Return the logistic loss for parameters `theta`.'''\n",
    "    return np.mean(np.log(1.0 + np.exp(-y*X.dot(theta))))\n",
    "\n",
    "def full_grad_f(theta):\n",
    "    '''Return the full gradient of the logistic loss `f` at point `theta`.'''\n",
    "    return (1./n)*(-y/(1 + np.exp(y * X.dot(theta)))).dot(X)\n",
    "\n",
    "def grad_f(theta, i):\n",
    "    '''Return the gradient of the logistic loss `f` at point `theta` for datum `i`.'''\n",
    "    return (1./n)*(-y[i]/(1 + np.exp(y[i] * X[i].dot(theta))))*X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations to be run\n",
    "max_iter = 200*n\n",
    "\n",
    "# Lists to save the sequences theta_hat, f(theta_hat), theta\n",
    "theta_history = []\n",
    "f_history = []\n",
    "\n",
    "# Stores the gradients and the mean gradient\n",
    "gradients = np.zeros((n,p))\n",
    "mean_gradient = np.zeros(p)\n",
    "\n",
    "# Step size\n",
    "L = np.linalg.norm((1./n)*X.T.dot(X), ord=2)\n",
    "step_size = 1. / (3*L)\n",
    "\n",
    "# Random initial point\n",
    "theta = np.random.randn(p)\n",
    "\n",
    "# SAGA Iterations\n",
    "for t in range(max_iter):\n",
    "    i = np.random.randint(0, n)\n",
    "    \n",
    "    # Computation of Gradients\n",
    "    gradient_i = # TODO\n",
    "    G = # TODO\n",
    "    \n",
    "    # Gradient Step\n",
    "    theta = # TODO\n",
    "    if t%(2*n) == 0:\n",
    "        theta_history.append(theta)\n",
    "        f_history.append(f(theta))\n",
    "    \n",
    "    # Storage\n",
    "    mean_gradient += (1./n) * (gradient_i - gradients[i])\n",
    "    gradients[i] = gradient_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVRG\n",
    "max_iter = 100\n",
    "max_iter_inner = 2*n\n",
    "\n",
    "# Lists to save the sequences theta_hat, f(theta_hat), theta\n",
    "f_SVRG_history = []\n",
    "theta_history = np.zeros((max_iter_inner, p))\n",
    "\n",
    "# Step size\n",
    "step_size = 0.005\n",
    "\n",
    "# Random initial point\n",
    "theta_hat = np.random.randn(p)\n",
    "\n",
    "# SVRG Iterations\n",
    "for t in range(max_iter):\n",
    "    \n",
    "    full_gradient_hat = full_grad_f(theta_hat)\n",
    "    theta = theta_hat\n",
    "    for k in range(max_iter_inner):\n",
    "        i = np.random.randint(0, n)\n",
    "        theta = theta - step_size * ( grad_f(theta, i)  - grad_f(theta_hat, i) + full_gradient_hat )\n",
    "        theta_history[k,:] = theta\n",
    "    \n",
    "    theta_hat = np.mean(theta_history, axis=0)\n",
    "    f_SVRG_history.append(f(theta_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the approximative minimum value of `f`\n",
    "min_f = np.min([np.min(f_history), np.min(f_SVRG_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now plot the convergence in terms of values of `f`\n",
    "\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.semilogy(f_SVRG_history-min_f, label='SVRG', lw=7)\n",
    "plt.semilogy(f_history-min_f, label='SAGA', lw=7)\n",
    "plt.grid(ls=':')\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Number of Iterations', fontsize=25)\n",
    "plt.ylabel('Approximation Error', fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
