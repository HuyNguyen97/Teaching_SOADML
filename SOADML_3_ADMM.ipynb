{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Optimization with the Alternating Direction Method of Multipliers\n",
    "\n",
    "In this lecture we will see the how the ADMM algorithm can be used for distributed optimization. Using the notatation from the course slides, we aim to solve the following optimization problem:\n",
    "\n",
    "![](ADMM_problem.png)\n",
    "\n",
    "\n",
    "The algorithm is described by the following iteration:\n",
    "\n",
    "![](ADMM_algorithm.png)\n",
    "\n",
    "\n",
    "__Structure of this session:__\n",
    "\n",
    "   * ADMM with 1 split (sequential algorithm).\n",
    "   * ADMM with 2 splits or more.\n",
    "   * We will see one application of the lasso: tomography reconstruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMM for the Lasso with a single machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use ADMM to solve a nonsmooth optimization problem.\n",
    "\n",
    "\n",
    "The Lasso is staistical model that has found many applications in machine learning, signal processing, biomedical imaging and genetics, to name a few. Given a regression dataset $X \\in \\mathbb{R}^{n \\times p}, y \\in \\mathbb{R}^n$, the Lasso is determined as the solution to the optimization problem\n",
    "\n",
    "$$\\text{argmin}_{\\theta} \\frac{1}{2n}\\|X \\theta - y\\|^2 + \\lambda \\|\\theta\\|_1 \\quad,$$\n",
    "where $\\lambda \\geq 0$ is a regularization parameter. The Lasso model is so useful because, given enough regularization, it gives solutions which are *sparse*, i.e., with many zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADMM formulation of the Lasso\n",
    "\n",
    "We can reformulate the Lasso problem in the following equivalent form\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{argmin}_{\\theta} \\frac{1}{2n}\\|X \\theta - y\\|^2 + \\lambda \\|\\rho\\|_1 \\quad,\\\\\n",
    "&\\text{ subject to }~ \\theta=\\rho\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now that we have written it in ADMM-friendly fashion, we can use the the above algorithm with $N=1$. This gives the following iteration:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\theta^{t+1} = \\text{argmin}_{\\theta} \\frac{1}{2n}\\|X \\theta - y\\|^2 + \\frac{\\tau}{2}\\|\\theta - \\rho^t + u^t\\|^2 \\\\\n",
    "&\\rho^{t+1} = \\text{argmin}_{\\theta} \\lambda \\|\\theta\\|_1 + \\frac{\\tau}{2} \\|\\theta - \\theta^{t+1} - u^t\\|^2\\\\\n",
    "& u^{t+1} = u^t + \\theta^{t+1} - \\rho^{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It turns that for this particular problem many of the steps have a closed form solution. Using this we have the following closed form updates:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\theta^{t+1} = \\frac{1}{n}(X^T X / n + \\tau I)^{-1}(X^T b / n + \\tau (\\rho^t - u^t)) \\\\\n",
    "&\\rho^{t+1} = (\\theta^{t+1} + u^t - \\frac{\\tau}{\\lambda})_+ - (\\frac{\\tau}{\\lambda} - \\theta^{t+1} - u^t)_+\\\\\n",
    "& u^{t+1} = u^t + \\theta^{t+1} - \\rho^{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $(\\cdot)_+$ denote the positive part, i.e., $(x)_+ := \\text{max}(x, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = 100, 100\n",
    "sigma = 0.1 # Noise level\n",
    "\n",
    "# Sparse coefficients: keep only 10 non-zero coefficients\n",
    "theta_star = np.random.randn(n_features)\n",
    "ind = np.arange(n_features)\n",
    "np.random.shuffle(ind)\n",
    "theta_star[ind[:-10]] = 0.\n",
    "\n",
    "# Simulate the data\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = X.dot(theta_star) + sigma*np.random.randn(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lasso(theta, X, y, lam):\n",
    "    '''\n",
    "        Return the value of the lasso for data `X`,`y`, parameter `theta` and regularization strength `lam`.\n",
    "    '''\n",
    "     # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_ADMM(X, y, lam, max_iter=100, tau=1.):\n",
    "    '''\n",
    "        Run the ADMM iterations for the Lasso.\n",
    "    '''\n",
    "    n_samples, n_features = X.shape\n",
    "    rho = np.zeros(n_features)\n",
    "    u = np.zeros(n_features)\n",
    "    \n",
    "    objective_history = []\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.sqrt(2*sigma*np.log(n_features)/n_samples)\n",
    "theta_hat, objective_history = lasso_ADMM(X, y, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.semilogy(objective_history-np.min(objective_history), lw=5, label='$f(\\\\theta) - \\min{f}$')\n",
    "plt.ylabel('Function values', fontsize=25)\n",
    "plt.xlabel('Number of iterations', fontsize=25)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso retrieve the real non-zero coefficients, provided $\\lambda$ is well chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detected non-zero coefficients: \", np.where(np.abs(theta_hat)>1e-10)[0])\n",
    "print(\"True non-zero coefficients:     \", np.where(np.abs(theta_star)>0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.scatter(range(n_features), theta_hat, alpha=0.95, s=150, label='$\\hat\\\\theta$')\n",
    "plt.scatter(range(n_features), theta_star, alpha=0.95, s=150, label='$\\\\theta^*$')\n",
    "plt.ylabel('Magnitude', fontsize=25)\n",
    "plt.xlabel('Coefficients', fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADMM for the Lasso with two machines\n",
    "\n",
    "We will now use ADMM to actually do some distributed computations. For this we can split the first subproblem into two ($N=2$) and using the ADMM algorithm specified at the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_ADMM_2(X, y, lam, max_iter=100, tau=1.):\n",
    "    \n",
    "    # Initialization\n",
    "    n_samples, n_features = X.shape\n",
    "    rho = np.zeros(n_features)\n",
    "    u_1 = np.zeros(n_features)\n",
    "    u_2 = np.zeros(n_features)\n",
    "    \n",
    "    objective_history = []\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.sqrt(2*sigma*np.log(n_features)/n_samples)\n",
    "theta, objective_history = lasso_ADMM_2(X, y, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.semilogy(objective_history-np.min(objective_history), lw=5, label='$f(\\\\theta) - \\min{f}$')\n",
    "plt.ylabel('Function values', fontsize=25)\n",
    "plt.xlabel('Number of iterations', fontsize=25)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso retrieve the real non-zero coefficients, provided $\\lambda$ is well chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detected non-zero coefficients: \", np.where(np.abs(theta_hat)>1e-10)[0])\n",
    "print(\"True non-zero coefficients:     \", np.where(np.abs(theta_star)>0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.scatter(range(n_features), theta_hat, alpha=0.95, s=150, label='$\\hat\\\\theta$')\n",
    "plt.scatter(range(n_features), theta_star, alpha=0.95, s=150, label='$\\\\theta^*$')\n",
    "plt.ylabel('Magnitude', fontsize=25)\n",
    "plt.xlabel('Coefficients', fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressive sensing application: tomography reconstruction with the Lasso\n",
    "\n",
    "\n",
    "This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in computed tomography (CT).\n",
    "\n",
    "Without any prior information on the sample, the number of projections required to reconstruct the image is of the order of the linear size l of the image (in pixels). For simplicity we consider here a sparse image, where only pixels on the boundary of objects have a non-zero value. Such data could correspond for example to a cellular material. Note however that most images are sparse in a different basis, such as the Haar wavelets. Only l/2 projections are acquired, therefore it is necessary to use prior information available on the sample (its sparsity): this is an example of compressive sensing.\n",
    "\n",
    "The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. **The resulting optimization problem is called the Lasso**.\n",
    "\n",
    "\n",
    "The reconstruction with L1 penalization should give a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (sklearn.linear_model.Ridge) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _weights(x, dx=1, orig=0):\n",
    "    x = np.ravel(x)\n",
    "    floor_x = np.floor((x - orig) / dx).astype(np.int64)\n",
    "    alpha = (x - orig - floor_x * dx) / dx\n",
    "    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n",
    "\n",
    "\n",
    "def _generate_center_coordinates(l_x):\n",
    "    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n",
    "    center = l_x / 2.\n",
    "    X += 0.5 - center\n",
    "    Y += 0.5 - center\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def build_projection_operator(l_x, n_dir):\n",
    "    X, Y = _generate_center_coordinates(l_x)\n",
    "    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n",
    "    data_inds, weights, camera_inds = [], [], []\n",
    "    data_unravel_indices = np.arange(l_x ** 2)\n",
    "    data_unravel_indices = np.hstack((data_unravel_indices,\n",
    "                                      data_unravel_indices))\n",
    "    for i, angle in enumerate(angles):\n",
    "        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n",
    "        inds, w = _weights(Xrot, dx=1, orig=X.min())\n",
    "        mask = np.logical_and(inds >= 0, inds < l_x)\n",
    "        weights += list(w[mask])\n",
    "        camera_inds += list(inds[mask] + i * l_x)\n",
    "        data_inds += list(data_unravel_indices[mask])\n",
    "    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n",
    "    return proj_operator\n",
    "\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    rs = np.random.RandomState(0)\n",
    "    n_pts = 36\n",
    "    x, y = np.ogrid[0:l, 0:l]\n",
    "    mask_outer = (x - l / 2.) ** 2 + (y - l / 2.) ** 2 < (l / 2.) ** 2\n",
    "    mask = np.zeros((l, l))\n",
    "    points = l * rs.rand(2, n_pts)\n",
    "    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n",
    "    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n",
    "    res = np.logical_and(mask > mask.mean(), mask_outer)\n",
    "    return np.logical_xor(res, ndimage.binary_erosion(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic images, and projections\n",
    "l = 25\n",
    "proj_operator = build_projection_operator(l, l // 2)\n",
    "data = generate_synthetic_data()\n",
    "proj = proj_operator * data.ravel()[:, np.newaxis]\n",
    "proj += 0.15 * np.random.randn(*proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(proj_operator.shape[0]):\n",
    "    plt.imshow(np.array(proj_operator.todense())[i].reshape(l,l), cmap=plt.cm.gray, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with L2 (Ridge) penalization\n",
    "rgr_ridge = Ridge(alpha=0.1)\n",
    "rgr_ridge.fit(proj_operator, proj.ravel())\n",
    "rec_l2 = rgr_ridge.coef_.reshape(l, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with L1 (Lasso) penalization\n",
    "rec_l1, objective_history = lasso_ADMM(np.array(proj_operator.todense()), proj.ravel(), lam=0.05, max_iter=1000)\n",
    "rec_l1 = rec_l1.reshape(l, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('Original image', fontsize=30)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('Ridge regression', fontsize=30)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('Lasso', fontsize=30)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.02, wspace=0.02, top=1, bottom=0, left=0, right=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
