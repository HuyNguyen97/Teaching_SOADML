{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install Autograd by typing ```conda install --channel conda-forge autograd``` in your console. We will test it on toy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import elementwise_grad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.abs(x)\n",
    "\n",
    "gradient_f = elementwise_grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,1000)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(f(x), label='$f$')\n",
    "plt.plot(gradient_f(x), label=\"$f'$\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Neural Network training: Geographical Origins of Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to train a two-layer feed-forward neural network in a regression problem. The data consists of music features and location. The goal is to retrieve the spatial origin of a music piece using the music features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Geographical Origins of Music dataset\n",
    "data = pd.read_table('Geographical Original of Music/default_plus_chromatic_features_1059_tracks.txt', sep=',', header=None)\n",
    "data.rename(columns={116:'Latitude', 117:'Longitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset in two : a train dataset with 80% of the data, and a test data set with the remaining 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-2], data.iloc[:,-2:], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the geographical repartition in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 13))\n",
    "\n",
    "# Train data\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.Robinson())\n",
    "\n",
    "ax.stock_img()\n",
    "ax.coastlines()\n",
    "\n",
    "ax.set_title('Train data')\n",
    "ax.scatter(y_train['Longitude'], y_train['Latitude'], c='r', transform=ccrs.Geodetic())\n",
    "\n",
    "# Test data\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=ccrs.Robinson())\n",
    "\n",
    "ax2.stock_img()\n",
    "ax2.coastlines()\n",
    "\n",
    "ax2.set_title('Test data')\n",
    "ax2.scatter(y_test['Longitude'], y_test['Latitude'], c='r', transform=ccrs.Geodetic())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of Longitude/Latitude with a 2-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the two-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the neural network layers\n",
    "size_features = X_train.shape[1]\n",
    "size_hidden_layer = # TODO: Try different values for the number of neurons in the hidden layer\n",
    "size_prediction = 2 # We want to predict the latitude and the longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the min and max of the Latitudes and Longitudes in the train dataset\n",
    "# This will be useful for rescaling the output of the neural network\n",
    "min_latitude = y_train['Latitude'].min()\n",
    "max_latitude = y_train['Latitude'].max()\n",
    "min_longitude = y_train['Longitude'].min()\n",
    "max_longitude = y_train['Longitude'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the targets and inputs of the neural network\n",
    "targets = np.array(y_train)\n",
    "inputs = np.array(X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(inputs, A1, A2, b1, b2, sgd=False):\n",
    "    '''Return the predicted value for features x with a 2-layer neural network of parameters A1 and A2 and biases b1 and b2.'''\n",
    "    assert inputs.shape[0] == A1.shape[1]\n",
    "    assert A2.shape == (2, A1.shape[0])\n",
    "    if sgd: # If only one data is to be computed\n",
    "        [latitude, longitude] = # TODO: return the objective value when SGD is run\n",
    "    else: # If the network has to compute several outputs \n",
    "        [latitude, longitude] = # TODO: return the objective value when GD is run\n",
    "    latitude = min_latitude + 0.5*(latitude+1)*(max_latitude - min_latitude)\n",
    "    longitude = min_longitude + 0.5*(longitude+1)*(max_longitude - min_longitude)\n",
    "    return np.array([latitude, longitude]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y_true):\n",
    "    '''Return the arc-cosine distance on the Earth between y_pred and y_true.'''\n",
    "    latitude_pred, longitude_pred = y_pred*np.pi/180.\n",
    "    latitude_true, longitude_true = y_true*np.pi/180.\n",
    "    return # TODO: search for the arc-cosine distance formula on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(weights):\n",
    "    '''Compute the objective to be minimized by the network using a gradient descent approach.'''\n",
    "    A1, A2, b1, b2 = weights\n",
    "    pred = network(inputs, A1, A2, b1, b2)\n",
    "    return # TODO: return the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_sgd(weights):\n",
    "    '''Compute the objective to be minimized by the network using a stochastic gradient descent approach.'''\n",
    "    A1, A2, b1, b2 = weights\n",
    "    i = np.random.randint(inputs.shape[1])\n",
    "    pred = network(inputs[:,i], A1, A2, b1, b2, sgd=True)\n",
    "    return # TODO: return the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_test(weights):\n",
    "    '''Compute the objective for the test dataset.'''\n",
    "    A1, A2, b1, b2 = weights\n",
    "    pred = network(X_test.T, A1, A2, b1, b2)\n",
    "    return # TODO: return the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fist use a Gradient Descent approach to minimize the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "epochs = 40 # Number of epochs to be ran by the Gradient Descent algorithm\n",
    "learning_rate = 0.00005\n",
    "\n",
    "# Initialize the weights and biases of the network\n",
    "A1 = np.random.randn(size_hidden_layer, size_features)\n",
    "A2 = np.random.randn(size_prediction, size_hidden_layer)\n",
    "b1 = np.random.randn(size_hidden_layer)\n",
    "b2 = np.random.randn(size_prediction)\n",
    "\n",
    "# Compute the gradient of the objective\n",
    "grad_loss = # TODO: use Autograd's function `grad`\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "print('Iteration', '|', 'Train loss', '|', 'Test loss')\n",
    "for t in range(epochs):\n",
    "    train_loss_history.append(objective([A1,A2,b1,b2]))\n",
    "    test_loss_history.append(objective_test([A1,A2,b1,b2]))\n",
    "    print(t, '        | ', int(train_loss_history[-1]), 'km   | ', int(test_loss_history[-1]), 'km')\n",
    "    \n",
    "    # Compute the gradients\n",
    "    grad_A1, grad_A2, grad_b1, grad_b2 = # TODO: use the preci-omputed gradient `grad_loss`\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    A1 = # TODO: use Gradient Descent update formula\n",
    "    A2 = # TODO: use Gradient Descent update formula\n",
    "    b1 = # TODO: use Gradient Descent update formula\n",
    "    b2 = # TODO: use Gradient Descent update formula\n",
    "\n",
    "# Save the weights and biases\n",
    "A1_GD, A2_GD, b1_GD, b2_GD = A1, A2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_objective = min([min(train_loss_history), min(test_loss_history)])\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.semilogy(train_loss_history-min_objective, lw=4, label='Train loss')\n",
    "plt.semilogy(test_loss_history-min_objective, lw=4, label='Test loss')\n",
    "plt.xlabel('Iterations', fontsize=25)\n",
    "plt.ylabel('Loss', fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe that the convergence of the Gradient Descent approach is slow, we also try a Stochastic Gradient Descent Approach. Since the objective is non convex, a stochastic approach will more likely avoid being stuck in local minima.\n",
    "We use a decaying learning rate in $1/\\sqrt{t}$ where $t$ is the number of iterations, and a Polyak-Ruppert averaging. This will always be the case in the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Stochastic Gradient Descent approach seems to converge way faster than the Gradient Descent approach in this case, we do not use the sae number of epochs : the SGD algorithm will use less data in this very application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "epochs = 10 # Number of passes over the data\n",
    "learning_rate = 0.00005\n",
    "\n",
    "# Initialize the weights and biases of the network\n",
    "A1 = np.random.randn(size_hidden_layer, size_features)\n",
    "A2 = np.random.randn(size_prediction, size_hidden_layer)\n",
    "b1 = np.random.randn(size_hidden_layer)\n",
    "b2 = np.random.randn(size_prediction)\n",
    "\n",
    "# Initialize the averaged weights and biases of the network\n",
    "A1_moy = np.copy(A1)\n",
    "A2_moy = np.copy(A2)\n",
    "b1_moy = np.copy(b1)\n",
    "b2_moy = np.copy(b2)\n",
    "\n",
    "# Compute the gradient of the objective\n",
    "grad_loss = # TODO\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print('Number of passes', '|', 'Train loss', '|', 'Test loss')\n",
    "for t in range(epochs*X_train.shape[0]):\n",
    "    if t % X_train.shape[0] == 0:\n",
    "        train_loss_history.append(objective([A1_moy,A2_moy,b1_moy,b2_moy]))\n",
    "        test_loss_history.append(objective_test([A1_moy,A2_moy,b1_moy,b2_moy]))\n",
    "        print(int(t/X_train.shape[0]), '             | ', int(train_loss_history[-1]), 'km   | ', int(test_loss_history[-1]), 'km')\n",
    "    \n",
    "    # Compute the gradients\n",
    "    grad_A1, grad_A2, grad_b1, grad_b2 = # TODO\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    A1 = # TODO: use SGD update formula, with strep_size decaying as 1/sqrt(t)\n",
    "    A2 = # TODO: use SGD update formula, with strep_size decaying as 1/sqrt(t)\n",
    "    b1 = # TODO: use SGD update formula, with strep_size decaying as 1/sqrt(t)\n",
    "    b2 = # TODO: use SGD update formula, with strep_size decaying as 1/sqrt(t)\n",
    "    \n",
    "    # Compute the averaged weights and biases (Polyak-Rupert)\n",
    "    A1_moy = (t*A1_moy + A1)/(t+1)\n",
    "    A2_moy = (t*A2_moy + A2)/(t+1)\n",
    "    b1_moy = (t*b1_moy + b1)/(t+1)\n",
    "    b2_moy = (t*b2_moy + b2)/(t+1)\n",
    "    \n",
    "    \n",
    "# Save the weights and biases\n",
    "A1_SGD, A2_SGD, b1_SGD, b2_SGD = A1_moy, A2_moy, b1_moy, b2_moy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_objective = min([min(train_loss_history), min(test_loss_history)])\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.semilogy(train_loss_history-min_objective, lw=4, label='Train loss')\n",
    "plt.semilogy(test_loss_history-min_objective, lw=4, label='Test loss')\n",
    "plt.xlabel('Iterations', fontsize=25)\n",
    "plt.ylabel('Loss', fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some random test data, and the predicted values for the GD approach and the SGD approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in both cases, the network fails to predict correctly the origin of a music piece. The train loss, as well as the test loss, are approximately of 4000km, which is clearly not precise enough. In particular, the network seems to always predict points at the center of the map (Europe/North Africa/Middle East), since this is a good strategy for musics coming from this area (predicting Europe instead of North Africa is cheap in terms of the arc-cosine loss), and not too costly for musics coming from South America/East Asia/Oceania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(X_test.shape[0])\n",
    "\n",
    "pred_SGD = network(X_test.iloc[i,:].T, A1_SGD, A2_SGD, b1_SGD, b2_SGD, True)\n",
    "pred_GD = network(X_test.iloc[i,:].T, A1_GD, A2_GD, b1_GD, b2_GD, True)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson())\n",
    "\n",
    "ax.stock_img()\n",
    "ax.coastlines()\n",
    "\n",
    "ax.scatter(y_test['Longitude'].iloc[i], y_test['Latitude'].iloc[i], c='r', s=100, transform=ccrs.Geodetic(), label='Real data')\n",
    "ax.scatter([pred_SGD[0]], [pred_SGD[1]], c='k', transform=ccrs.Geodetic(), s=100, label='Predicted value with SGD')\n",
    "ax.scatter([pred_GD[0]], [pred_GD[1]], c='b', transform=ccrs.Geodetic(), s=100, label='Predicted value with GD')\n",
    "\n",
    "ax.set_title('Test data N°'+str(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to predict a heatmap instead of a fixed location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the music data\n",
    "data = pd.read_table('Geographical Original of Music/default_plus_chromatic_features_1059_tracks.txt', sep=',', header=None)\n",
    "data.drop(data.columns[-2:], axis=1, inplace=True)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geographical maps\n",
    "maps = np.array(pd.read_table('heatmap.txt', sep='   ', header=None, engine='python'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate music data and geographical maps\n",
    "data = np.concatenate([data, maps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two : train dataset (80% of the data) and a test dataset (the 20% remaining)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:,:-400], data[:,-400:], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random data\n",
    "i = np.random.randint(y_train.shape[0])\n",
    "plt.imshow(y_train[i].reshape(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the neural network layers\n",
    "size_features = X_train.shape[1]\n",
    "size_hidden_layer = # TODO: Try different values for the number of neurons in the hidden layer\n",
    "size_prediction = 400 # We want to predict a 20x20 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the targets and inputs of the neural network\n",
    "targets = np.array(y_train)\n",
    "inputs = np.array(X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Return the softmax of vector x.'''\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(inputs, A1, A2, b1, b2, sgd=False):\n",
    "    '''Return the predicted value for features x with a 2-layer neural network of weights A1 and A2 and biases b1 and b2.'''\n",
    "    assert inputs.shape[0] == A1.shape[1]\n",
    "    assert A2.shape == (400, A1.shape[0])\n",
    "    if sgd: # If there is only one data to be predicted\n",
    "        return # TODO: return the prediction of the network\n",
    "    else: # If there is several data to be passed to the network\n",
    "        return # TODO: return the prediction of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent with a KL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p, q):\n",
    "    '''Return the entropy of p wrt q.'''\n",
    "    tol = 0.001\n",
    "    assert p.shape[0] == q.shape[0]\n",
    "    assert np.prod(q>0) == 1\n",
    "    return -np.sum(p*np.log(q+tol))\n",
    "    \n",
    "def objective_KL(weights):\n",
    "    '''Compute the objective to be minimized with a KL (entropy) loss and a gradient descent approach.'''\n",
    "    A1, A2, b1, b2 = weights\n",
    "    pred = network(inputs, A1, A2, b1, b2)\n",
    "    return # TODO: return the KL loss\n",
    "\n",
    "def objective_KL_sgd(weights):\n",
    "    '''Compute the objective to be minimized with a KL (entropy) loss and a stochastic gradient descent approach.'''\n",
    "    A1, A2, b1, b2 = weights\n",
    "    i = np.random.randint(inputs.shape[1])\n",
    "    pred = network(inputs[:,i], A1, A2, b1, b2, True)\n",
    "    return # TODO: return the KL loss\n",
    "\n",
    "def objective_KL_test(weights):\n",
    "    '''Compute the objective with a KL (entropy) loss for the test dataset.'''\n",
    "    A1, A2, b1, b2 = weights\n",
    "    pred = network(X_test.T, A1, A2, b1, b2)\n",
    "    return # TODO: return the KL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "epochs = 50 # Number of epochs to be run\n",
    "learning_rate = 2.\n",
    "\n",
    "# Initialize the network's weigths and biases\n",
    "A1 = 0.01*np.random.randn(size_hidden_layer, size_features)\n",
    "A2 = 0.01*np.random.randn(size_prediction, size_hidden_layer)\n",
    "b1 = 0.01*np.random.randn(size_hidden_layer)\n",
    "b2 = 0.01*np.random.randn(size_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the gradient of the objective\n",
    "grad_loss = # TODO: use Autograd\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print('Iteration', '|', 'Train loss', '         |', 'Test loss')\n",
    "for t in range(epochs):\n",
    "    train_loss_history.append(objective_KL([A1,A2,b1,b2]))\n",
    "    test_loss_history.append(objective_KL_test([A1,A2,b1,b2]))\n",
    "    print(t, '        | ', train_loss_history[-1], ' | ', test_loss_history[-1])\n",
    "    \n",
    "    # Compute the gradients of the parameters\n",
    "    grad_A1, grad_A2, grad_b1, grad_b2 = # TODO\n",
    "    \n",
    "    # Run the descent step\n",
    "    A1 = # TODO\n",
    "    A2 = # TODO\n",
    "    b1 = # TODO\n",
    "    b2 = # TODO\n",
    "\n",
    "# Save the parameters\n",
    "A1_KL_GD, A2_KL_GD, b1_KL_GD, b2_KL_GD = A1, A2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_objective = min([min(train_loss_history), min(test_loss_history)])\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.semilogy(train_loss_history-min_objective, lw=4, label='Train loss')\n",
    "plt.semilogy(test_loss_history-min_objective, lw=4, label='Test loss')\n",
    "plt.xlabel('Iterations', fontsize=25)\n",
    "plt.ylabel('Loss', fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent with a KL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "epochs = 50 # Number of passes over the data\n",
    "learning_rate = 2.\n",
    "\n",
    "# Initialize the network's parameters\n",
    "A1 = 0.01*np.random.randn(size_hidden_layer, size_features)\n",
    "A2 = 0.01*np.random.randn(size_prediction, size_hidden_layer)\n",
    "b1 = 0.01*np.random.randn(size_hidden_layer)\n",
    "b2 = 0.01*np.random.randn(size_prediction)\n",
    "\n",
    "# Initialize the averaged parameters\n",
    "A1_moy = np.copy(A1)\n",
    "A2_moy = np.copy(A2)\n",
    "b1_moy = np.copy(b1)\n",
    "b2_moy = np.copy(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the gradient of the SGD objective\n",
    "grad_loss = # TODO: use Autograd\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "print('Number of passes', '|', 'Train loss', '         |', 'Test loss')\n",
    "for t in range(epochs*X_train.shape[0]):\n",
    "    if t % X_train.shape[0] == 0:\n",
    "        train_loss_history.append(objective_KL([A1_moy,A2_moy,b1_moy,b2_moy]))\n",
    "        test_loss_history.append(objective_KL_test([A1_moy,A2_moy,b1_moy,b2_moy]))\n",
    "        print(int(t/X_train.shape[0]), '             | ', train_loss_history[-1], ' | ', test_loss_history[-1])\n",
    "    \n",
    "    # Compute the gradients of the parameters\n",
    "    grad_A1, grad_A2, grad_b1, grad_b2 = # TODO\n",
    "    \n",
    "    # Run the descent step\n",
    "    A1 = # TODO\n",
    "    A2 = # TODO\n",
    "    b1 = # TODO\n",
    "    b2 = # TODO\n",
    "    \n",
    "    # Run the averaging step\n",
    "    A1_moy = (t*A1_moy + A1)/(t+1)\n",
    "    A2_moy = (t*A2_moy + A2)/(t+1)\n",
    "    b1_moy = (t*b1_moy + b1)/(t+1)\n",
    "    b2_moy = (t*b2_moy + b2)/(t+1)\n",
    "    \n",
    "# Save the parameters\n",
    "A1_KL_SGD, A2_KL_SGD, b1_KL_SGD, b2_KL_SGD = A1_moy, A2_moy, b1_moy, b2_moy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_objective = min([min(train_loss_history), min(test_loss_history)])\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.semilogy(train_loss_history-min_objective, lw=4, label='Train loss')\n",
    "plt.semilogy(test_loss_history-min_objective, lw=4, label='Test loss')\n",
    "plt.xlabel('Iterations', fontsize=25)\n",
    "plt.ylabel('Loss', fontsize=25)\n",
    "plt.legend(fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some random test data, and the predicted values for the GD approach and the SGD approach (for the KL/entropy loss). As the test losses show, the Stochastic Gradient Approach leads to slightly better predictions than the GD approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(X_test.shape[0])\n",
    "\n",
    "pred_GD = network(X_test[i], A1_KL_GD, A2_KL_GD, b1_KL_GD, b2_KL_GD, True)\n",
    "pred_SGD = network(X_test[i], A1_KL_SGD, A2_KL_SGD, b1_KL_SGD, b2_KL_SGD, True)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.imshow(y_test[i].reshape(20,20))\n",
    "ax.set_title('Real data')\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.imshow(pred_GD.reshape(20,20))\n",
    "ax2.set_title('Prediction with GD')\n",
    "\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.imshow(pred_SGD.reshape(20,20))\n",
    "ax3.set_title('Prediction with SGD')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
